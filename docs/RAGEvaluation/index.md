---
title: RAG Evaluation
---

## Evaluation Approaches


## Evaluation Frameworks & Tools

## Tools

| Tool | Usage |
|------|-------|
| [Arthur Bench](https://www.arthur.ai/product/bench) | help teams evaluate the different LLM options out there in a quick, easy and consistent way |
| [Evaluate](https://huggingface.co/docs/evaluate/index) | A Hugging Face library for easily evaluating machine learning models and datasets. |


### Evaluation Metrics

| Metric | Summary | References |
| -------|---------|------------|
| [BERTScore](https://openreview.net/pdf?id=SkeHuCVFDr) | BERTScore is a metric for automatic evaluation of machine translation that calculates the similarity between a machine translation output and a reference translation using sentence representation. BERTScore was invented as an improvement on n-gram-based metrics like BLEU.| [Hugging Face](https://huggingface.co/spaces/evaluate-metric/bertscore) |
